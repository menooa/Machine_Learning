{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization is just for strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings that would normally be displayed during execution\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Menoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Menoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize (a Word-based tokenizer) \n",
    "import nltk  # Natural Language Toolkit (a Python library)\n",
    "nltk.download(\"punkt\")  # Downloads the Punkt tokenizer models, which are used for sentence and word tokenization.\n",
    "nltk.download(\"punkt_tab\")  # punkt_tab could contain additional pre-trained tokenization rules or enhancements for punkt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is a sample sentence for word tokenization.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', 'thery', 'how', 'you', 'doin?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or simply use built-in split() for basic tokenization\n",
    "text = 'Hey thery how you doin?'\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't help the dog. Can't you do it? Don't be afraid if you are.\n",
      "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n",
      "I PRON nsubj\n",
      "could AUX aux\n",
      "n't PART neg\n",
      "help VERB ROOT\n",
      "the DET det\n",
      "dog NOUN dobj\n",
      ". PUNCT punct\n",
      "Ca AUX aux\n",
      "n't PART neg\n",
      "you PRON nsubj\n",
      "do VERB ROOT\n",
      "it PRON dobj\n",
      "? PUNCT punct\n",
      "Do AUX aux\n",
      "n't PART neg\n",
      "be AUX ROOT\n",
      "afraid ADJ acomp\n",
      "if SCONJ mark\n",
      "you PRON nsubj\n",
      "are AUX advcl\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "# rule-based tokenizer (spaCy tokenizer)\n",
    "# helps computers understand and analyze text\n",
    "import spacy\n",
    "\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the English NLP model\n",
    "# en_core_web_sm: a small English NLP model that helps computers understand and analyze text.\n",
    "doc = nlp(text)  # Process the text using spaCy\n",
    "print(doc)\n",
    "\n",
    "# make a list of tokens\n",
    "token_list = [token.text for token in doc]\n",
    "# .text: the word of the token\n",
    "print(token_list)\n",
    "\n",
    "# show token details\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "    # .pos_: grammatical category of the word (e.g., noun, verb, adjective, etc.).\n",
    "    # .dep_: shows the relationship of the word with other words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', 'token', '##ization']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subword-based tokenizer (BertTokenizer)\n",
    "# Note that it treats composite words as separate tokens\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'B', 'M', ' ', 't', 'a', 'u', 'g', 'h', 't', ' ', 'm', 'e', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n']\n"
     ]
    }
   ],
   "source": [
    "# for character-based tokenizer we simply use list?\n",
    "text = \"IBM taught me tokenization\"\n",
    "char_tokens = list(text)\n",
    "print(char_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['笆！BM', '笆》aught', '笆［e', '笆》oken', 'ization', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XLNetTokenizer uses a subword-based tokenizer, specifically SentencePiece with an Unigram Language Model\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with PyTorch\n",
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'how', 'you', 'doin', '!']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "# for this change python version to 3.10.7\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# basic_english: lowercases text and splits it into words\n",
    "tokenizer('HeLlo how You doiN!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing\n",
    "tokenizer(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'to', 'nlp']\n",
      "['basics', 'of', 'pytorch']\n",
      "['nlp', 'techniques', 'for', 'text', 'classification']\n"
     ]
    }
   ],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _,text in data_iter:  # so in tuples \"_\" is the number and \"text\" is the text\n",
    "        yield tokenizer(text)\n",
    "        # we need to get all \"for\"s, so we use yield (it keeps them as a generator object)\n",
    "        # return just gives the first \"for\" (and also gives error in next())\n",
    "\n",
    "my_iterator = yield_tokens(dataset)\n",
    "\n",
    "print(next(my_iterator))\n",
    "print(next(my_iterator))\n",
    "print(next(my_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word-to-index mapping.\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "# build_vocab_from_iterator() takes the tokenized text iterator as input -> creates a word-to-index mapping.\n",
    "# This mapping is used to convert text into numerical format for ML models.\n",
    "# Unknown words get mapped to <unk> (index 0).\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# vocab[\"<unk>\"]: Retrieves the index of the special token <unk>, which represents unknown words.\n",
    "# vocab.set_default_index(0): \n",
    "# Ensures that any word not found in the vocabulary is automatically assigned the <unk> index.\n",
    "# Prevents errors when looking up missing words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['machine', 'translation', 'with', 'nlp']\n",
      "Token Indices: [5, 8, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "# to get a list of tokenz of each tuple and a list of their indexs by each run of this cell\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices\n",
    "\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)\n",
    "# why skips some lines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\"IBM taught me tokenization\", \n",
    "         \"Special tokenizers are ready and they will blow your mind\", \n",
    "         \"just saying hi!\"]\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "# <unk>: \"Unknown\" token\n",
    "# <pad>: \"Padding\" token\n",
    "# <bos>: \"Beginning of Sequence\" token\n",
    "# <eos>: \"End of Sequence\" token\n",
    "\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "# spacy: 'spacy' indicates you want to use spaCy's tokenization method\n",
    "# language='en_core_web_sm': to use spaCy's small English language model\n",
    "\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    # eg it will be [\"<bos>\", \"hello\", \"world\", \"<eos>\"]\n",
    "    # Help the model identify where sequences start and end\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "    # max_length - len(tokens[i]): calculates how many padding tokens are needed\n",
    "    # for example If we have:\n",
    "    # tokens[i] = [\"hello\", \"world\"]\n",
    "    # max_length = 5\n",
    "    # tokens[i] = tokens[i] + ['<pad>'] * (5 - 2)\n",
    "    # The result would be:\n",
    "    # tokens[i] = [\"hello\", \"world\", \"<pad>\", \"<pad>\", \"<pad>\"]\n",
    "\n",
    "print(\"Lines after adding special tokens:\\n\", tokens)\n",
    "\n",
    "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line = \"I learned about embeddings and attention mechanisms.\"\n",
    "\n",
    "tokenized_new_line = tokenizer_en(new_line)\n",
    "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']\n",
    "\n",
    "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))\n",
    "\n",
    "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
    "\n",
    "print(\"Token IDs for new line:\", new_line_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
