{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clinical_data = pd.read_csv('clinical_data.csv')\n",
    "\n",
    "# to change delimiter from \",\" to \"|\"\n",
    "clinical_data.to_csv('clinical_data_pipe.csv', sep='|', index=False)\n",
    "# we use \"|\" instead of \",\" as delimiter so we won't accidentally seperate a phrase that has ',' inside\n",
    "\n",
    "clinical_data = pd.read_csv('clinical_data_pipe.csv', delimiter='|')\n",
    "\n",
    "# Data cleaning\n",
    "import re\n",
    "def deidentify(text):\n",
    "    # Remove MRN (medical record number), names, dates, phone numbers, etc.\n",
    "    text = re.sub(r'MRN:?\\s*\\d+', '[MRN]', text)\n",
    "    # eg \"Patient MRN: 12345\" => \"Patient [MRN]\"\n",
    "    # re.sub() is a Python function that substitutes text matching a pattern with replacement text\n",
    "    # r'MRN:?\\s*\\d+' is the pattern to search for:\n",
    "    # MRN - the literal characters \"MRN\"\n",
    "    # :? - an optional colon (the ? makes the : optional)\n",
    "    # \\s* - zero or more whitespace characters (spaces, tabs, etc.)\n",
    "    # \\d+ - one or more digits\n",
    "    # '[MRN]' is the replacement text\n",
    "    text = re.sub(r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{1,2},? \\d{4}\\b', '[DATE]', text)\n",
    "    # \\b (word boundry): a position in text where a word character (like a letter, number, or underscore) meets a non-word character (like a space, punctuation mark, or the start/end of text). we put \\b at the beginnig and end to ensure that the patern is not part of another word\n",
    "    # [a-z]*: to capture full name of the months as well\n",
    "    # to replace (eg, \"Jan 1, 2023\" or \"February 14 2022\") with [Date]\n",
    "    # \\d{1,2}: One or two digits for the day (1-31)\n",
    "    # (?:...): to make the month as non capturing group, so we can't index it later (don't get the point at all!!) \n",
    "    text = re.sub(r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n",
    "    # My number is 555.123.4567\" â†’ \"My number is [PHONE]\"\n",
    "    # [-.\\s]: - or . or space (like (-|.|\\s) )\n",
    "    # The character class [-.\\s] is slightly more efficient in most regex engines\n",
    "    # The group with alternation (-|.|\\s) creates a capturing group unless you use (?:-|.|\\s)\n",
    "\n",
    "    return text\n",
    "\n",
    "# apply the function to the whole column\n",
    "clinical_data['clinical_notes'] = clinical_data['clinical_notes'].apply(deidentify)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(clinical_data, test_size=0.2,stratify=clinical_data['specialities'], random_state=42)\n",
    "# stratify=...: ensures that the proportion of each medical specialty is maintained in both your training and test datasets.\n",
    "# For example, if your original dataset contains:\n",
    "# 50% Cardiology, 30% Neurology, 20% Dermatology\n",
    "# Then both your training and test sets will maintain these same proportions.\n",
    "# important in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# didn't learn from this point\n",
    "# so didn't learn after Classifying_Document.ipynb in 8th folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a medical terminology list (simplified example)\n",
    "medical_terms = ['arrhythmia', 'hypertension', 'tachycardia', 'lesion', 'melanoma', \n",
    "                 'seizure', 'migraine', 'gastritis', 'reflux', 'fracture', 'arthritis']\n",
    "\n",
    "# Use medical stopwords and include bigrams/trigrams which are important in medical text\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 3),  # Include phrases up to 3 words\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "y_train = train_data['specialty']\n",
    "y_test = test_data['specialty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Handle imbalanced classes (some specialties might have fewer notes)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert data to HuggingFace format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Use BioBERT, which is pretrained on biomedical literature\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"dmis-lab/biobert-v1.1\", \n",
    "    num_labels=len(specialties)\n",
    ")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Configure training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./medical_classifier_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples where model was most confident but wrong\n",
    "from scipy.special import softmax\n",
    "\n",
    "def get_critical_errors(texts, true_labels, predicted_labels, probabilities, top_n=10):\n",
    "    errors = []\n",
    "    for i, (text, true, pred, probs) in enumerate(zip(texts, true_labels, predicted_labels, probabilities)):\n",
    "        if true != pred:\n",
    "            confidence = max(softmax(probs))\n",
    "            errors.append((confidence, text, true, pred))\n",
    "    \n",
    "    # Sort by confidence (most confident mistakes first)\n",
    "    return sorted(errors, key=lambda x: x[0], reverse=True)[:top_n]\n",
    "\n",
    "# These would be reviewed by clinical experts\n",
    "critical_errors = get_critical_errors(\n",
    "    test_data['text'],\n",
    "    y_test, \n",
    "    predictions,\n",
    "    model.predict_proba(X_test)\n",
    ")\n",
    "\n",
    "for confidence, text, true, pred in critical_errors:\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print(f\"True: {true}, Predicted: {pred}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import secrets\n",
    "import logging\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configure secure logging (no PHI)\n",
    "logging.basicConfig(filename='classifier_audit.log', level=logging.INFO)\n",
    "\n",
    "# Authentication for internal systems\n",
    "API_KEYS = {\"internal_ehr_system\": secrets.token_hex(32)}\n",
    "\n",
    "@app.route('/classify_note', methods=['POST'])\n",
    "def classify_note():\n",
    "    # Verify API key\n",
    "    api_key = request.headers.get('X-API-Key')\n",
    "    if api_key not in API_KEYS.values():\n",
    "        logging.warning(f\"Unauthorized access attempt from {request.remote_addr}\")\n",
    "        return jsonify({\"error\": \"Unauthorized\"}), 401\n",
    "    \n",
    "    data = request.json\n",
    "    text = data['clinical_note']\n",
    "    note_id = data.get('note_id', 'unknown')  # For audit trail\n",
    "    \n",
    "    # Deidentify before processing\n",
    "    text = deidentify(text)\n",
    "    \n",
    "    # Process and predict\n",
    "    features = vectorizer.transform([text])\n",
    "    specialty = model.predict(features)[0]\n",
    "    confidence = float(max(model.predict_proba(features)[0]))\n",
    "    \n",
    "    # Log the prediction (without PHI)\n",
    "    logging.info(f\"Note ID: {note_id}, Predicted: {specialty}, Confidence: {confidence:.2f}\")\n",
    "    \n",
    "    return jsonify({\n",
    "        'note_id': note_id,\n",
    "        'predicted_specialty': specialty,\n",
    "        'confidence': confidence,\n",
    "        'recommended_review': confidence < 0.7  # Flag low confidence predictions\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run with HTTPS in production\n",
    "    app.run(host='0.0.0.0', port=5000, ssl_context='adhoc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation for FDA compliance (if used for clinical decision support)\n",
    "def generate_model_card():\n",
    "    model_info = {\n",
    "        \"model_name\": \"Clinical Specialty Classifier v1.0\",\n",
    "        \"purpose\": \"Assist in routing clinical notes to appropriate specialists\",\n",
    "        \"training_data\": \"10,000 deidentified clinical notes from 2020-2023\",\n",
    "        \"performance\": {\n",
    "            \"overall_accuracy\": 0.89,\n",
    "            \"cardiology_precision\": 0.92,\n",
    "            \"cardiology_recall\": 0.88,\n",
    "            # Add metrics for all specialties\n",
    "        },\n",
    "        \"limitations\": \"Not intended for emergency triage. Clinical oversight required.\",\n",
    "        \"validation_method\": \"5-fold cross validation + expert review\",\n",
    "        \"last_updated\": \"2024-02-25\"\n",
    "    }\n",
    "    \n",
    "    with open(\"model_documentation.json\", \"w\") as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "\n",
    "# Regular retraining schedule\n",
    "def schedule_retraining():\n",
    "    from apscheduler.schedulers.background import BackgroundScheduler\n",
    "    \n",
    "    scheduler = BackgroundScheduler()\n",
    "    # Retrain every 3 months with new clinical data\n",
    "    scheduler.add_job(retrain_model, 'interval', months=3)\n",
    "    scheduler.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
